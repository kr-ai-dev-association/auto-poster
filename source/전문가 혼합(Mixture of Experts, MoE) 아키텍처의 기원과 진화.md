전문가 혼합(Mixture of Experts, MoE) 아키텍처의 기원과 진화: 글로벌 AI 패권 전쟁과 인재 흐름에 대한 심층 보고서1. 서론: 조건부 연산의 르네상스와 효율성 혁명인공지능(AI)의 역사는 연산 능력(Compute)과 데이터의 규모를 확장하려는 시도와, 그로 인해 발생하는 막대한 비용을 효율화하려는 시도 간의 끊임없는 변증법적 발전 과정으로 요약할 수 있습니다. 2020년대 초반, GPT-3로 대표되는 '거대 언어 모델(LLM)'의 시대가 열리면서 모델의 파라미터(Parameter) 수는 기하급수적으로 증가했습니다. 그러나 모든 입력 데이터에 대해 모델의 모든 파라미터를 활성화하는 '밀집(Dense)' 모델 방식은 추론 비용의 급격한 상승과 하드웨어의 물리적 한계라는 벽에 부딪혔습니다. 이러한 배경에서, 입력 토큰(Token)의 특성에 따라 필요한 전문가(Expert) 네트워크만을 선별적으로 활성화하는 '전문가 혼합(Mixture of Experts, 이하 MoE)' 아키텍처가 게임 체인저로 부상했습니다.본 보고서는 단순한 기술 분석을 넘어, MoE 아키텍처의 탄생부터 현재에 이르기까지의 30년 역사를 망라합니다. 1991년 인지과학적 호기심에서 출발한 이 아이디어가 어떻게 2017년 구글 브레인(Google Brain)을 거쳐 2024년 중국의 딥시크(DeepSeek)와 프랑스의 미스트랄(Mistral)에 의해 '모델 전쟁(Model Wars)'의 핵심 무기로 재탄생했는지를 추적합니다. 특히 사용자의 요청에 따라 딥시크의 창업자 **량원펑(Liang Wenfeng)**의 구현 전략을 해부하고, 라마(Llama) 사태 이후 촉발된 오픈소스 생태계 내 프랑스와 인도 연구자들의 역할을 조명하며, 이 모든 기술적 성취가 어떻게 마이크로소프트 리서치 아시아(MSRA)라는 허브를 통해 중국의 AI 굴기와 연결되는지 지정학적 관점에서 분석합니다.2. MoE의 고고학: 1991년의 선구자들과 이론적 토대현대의 딥시크나 미스트랄이 사용하는 MoE 아키텍처를 이해하기 위해서는, GPU 클러스터가 존재하지 않던 시절, 인간 뇌의 모듈성(Modularity)을 모방하고자 했던 초기 연결주의(Connectionism) 시대의 연구로 거슬러 올라가야 합니다.2.1 최초의 논문: "Adaptive Mixtures of Local Experts" (1991)MoE의 개념적 기원이 되는 논문은 1991년 Neural Computation 저널에 게재된 **"Adaptive Mixtures of Local Experts"**입니다. 이 논문은 하나의 거대한 신경망이 모든 종류의 데이터 패턴을 학습하려 할 때 발생하는 '간섭(Crosstalk)' 문제를 해결하기 위해 제안되었습니다.1핵심 아이디어: 데이터 공간을 여러 개의 하위 영역으로 분할하고, 각 영역을 전담하는 '로컬 전문가(Local Expert)' 네트워크들을 배치합니다. 그리고 입력 데이터가 들어올 때, 어떤 전문가가 이 데이터를 처리하는 것이 가장 적합한지를 확률적으로 결정하는 '게이팅 네트워크(Gating Network)'를 둡니다. 이는 현대 MoE의 '라우터(Router)' 개념의 시초입니다.2.2 저자 심층 분석: 거인들의 어깨이 논문의 저자들은 오늘날 AI 교과서에 등장하는 전설적인 인물들로 구성되어 있으며, 이들의 당시 역할과 배경은 MoE 사상의 뿌리를 이해하는 데 필수적입니다.저자 (Author)당시 소속 및 역할기여 및 현대적 의의Robert A. Jacobs매사추세츠 대학교 / 로체스터 대학교제1저자 & 교신저자. 인지과학적 관점에서 '태스크 분해(Task Decomposition)' 이론을 주도했습니다. 하나의 시스템이 복잡한 문제를 해결할 때, 이를 작은 문제로 쪼개어 해결하는 것이 생물학적으로 타당하다는 가설을 수학적으로 모델링했습니다. 1Michael I. JordanMIT (후에 UC 버클리)머신러닝의 통계적 기초를 다진 석학이자 앤드류 응(Andrew Ng), 요슈아 벤지오(Yoshua Bengio)의 스승. 그는 MoE를 '최대 우도 추정(Maximum Likelihood Estimation)' 프레임워크 내에서 해석하고, EM(Expectation-Maximization) 알고리즘과 유사한 학습 절차를 정립하여 수학적 엄밀성을 부여했습니다. 2Steven J. Nowlan토론토 대학교**경쟁 학습(Competitive Learning)**의 전문가. 전문가 네트워크들이 서로 데이터를 차지하기 위해 '경쟁'하는 구조를 설계했습니다. 그의 1990년 연구인 "Maximum Likelihood Competitive Learning"은 MoE의 직접적인 전신(Precursor)입니다. 3Geoffrey E. Hinton토론토 대학교AI의 대부(Godfather of AI). 힌튼 교수는 역전파(Backpropagation) 알고리즘을 통해 신경망을 학습시키는 과정에서, 모든 뉴런이 모든 작업에 관여하는 비효율성을 직관적으로 파악했습니다. 그는 1991년 논문뿐만 아니라, 26년 후인 2017년 구글 브레인에서 MoE를 부활시키는 논문에도 저자로 참여하며, 이 아키텍처의 과거와 현재를 잇는 유일한 연결고리가 됩니다. 1통찰(Insight): 1991년의 MoE는 '소프트 게이팅(Soft Gating)' 방식을 사용했습니다. 즉, 모든 전문가가 입력에 대해 조금씩 기여하고, 그 가중합을 출력으로 내보내는 방식이었습니다. 이는 이론적으로는 우아했으나, 모든 전문가를 연산해야 했기에 연산량 절감 효과는 없었습니다. 진정한 '희소성(Sparsity)'의 개념은 이후 하드웨어의 발전과 함께 등장하게 됩니다.3. 조건부 연산의 부활: 구글 브레인과 2017년의 전환점MoE가 이론적 호기심에서 실용적인 대규모 아키텍처로 변모한 시점은 2017년입니다. 구글 브레인 팀은 LSTM(Long Short-Term Memory) 기반의 번역 모델 용량을 극적으로 늘리기 위해 1991년의 아이디어를 재소환했습니다.3.1 "Outrageously Large Neural Networks" (2017)논문: Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (ICLR 2017)저자: Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean.6이 논문은 현대 MoE의 표준인 '희소 게이팅(Sparsely-Gated)' 메커니즘을 정립했습니다. 수천 개의 전문가가 있더라도, 입력 토큰 하나당 상위 k개(Top-k, 보통 k=1 또는 2)의 전문가만 활성화하고 나머지는 0으로 처리하는 방식입니다. 이를 통해 모델의 파라미터 수를 수십, 수백 배 늘리면서도 추론 비용(FLOPs)은 일정하게 유지하는 마법 같은 확장이 가능해졌습니다.Noam Shazeer의 역할: 제1저자인 놈 샤지어(Noam Shazeer)는 이후 'Attention Is All You Need' 논문의 공저자이기도 하며, 현재 Character.AI의 창업자입니다. 그는 2017년 논문에서 미분 불가능한 Top-k 선택 과정을 우회하기 위한 'Noisy Top-k Gating'을 고안해냈습니다. 54. 량원펑(Liang Wenfeng)과 딥시크(DeepSeek)의 구현 전략: 제약이 낳은 혁신사용자가 가장 중점적으로 질문한 량원펑의 구현 방식과 딥시크의 기술적 차별점에 대한 심층 분석입니다. 딥시크의 성공은 단순히 오픈소스 코드를 가져다 쓴 것이 아니라, 미국의 대중국 반도체 제재라는 극한의 제약 조건 속에서 탄생한 '생존형 최적화'의 결과물입니다.4.1 량원펑(Liang Wenfeng): 금융 공학자에서 AI 아키텍트로량원펑은 전통적인 AI 연구자 출신이 아닙니다. 그는 **절강대학교(Zhejiang University)**에서 정보통신공학을 전공한 뒤, 2015년 양적완화 헤지펀드인 **High-Flyer(환방량화, 幻方量化)**를 설립한 금융 공학자입니다.9알고리즘 트레이딩(Quant)과 AI의 접점: 퀀트 트레이딩의 핵심은 '효율성'과 '레이턴시(Latency)'입니다. 제한된 시간과 컴퓨팅 자원 내에서 시장의 패턴을 찾아내야 하는 퀀트의 철학은, 제한된 GPU 자원 내에서 최고의 지능을 뽑아내야 하는 LLM 학습과 일맥상통합니다.자본의 독립성: 량원펑은 외부 VC의 간섭 없이, 헤지펀드에서 벌어들인 막대한 수익을 AI 인프라에 재투자했습니다. 보도에 따르면 그는 2021년 미국의 반도체 제재가 강화되기 직전, 엔비디아 A100 GPU 약 10,000장을 선제적으로 확보하며 독자적인 'DeepSeek' 클러스터를 구축했습니다.114.2 딥시크 MoE(DeepSeekMoE)의 기술적 구현딥시크의 연구팀(Daya Guo 등이 주도)은 기존의 서구권 MoE(구글 GShard, 미스트랄 Mixtral)가 가진 비효율성을 간파하고, 이를 개선하기 위해 두 가지 핵심 전략을 구현했습니다.12전략 1: 미세 입자 전문가 분할 (Fine-Grained Expert Segmentation)기존의 MoE(예: Mixtral 8x7B)는 전문가의 수가 적고(8개), 각 전문가의 크기가 컸습니다. 이는 '지식의 혼종성(Knowledge Hybridity)' 문제를 야기합니다. 즉, 하나의 전문가가 '코딩'과 '프랑스 역사' 같은 이질적인 지식을 억지로 함께 학습해야 하는 비효율이 발생합니다.구현: 딥시크는 전문가를 매우 잘게 쪼갰습니다. DeepSeek-V3의 경우 총 256개의 전문가를 두지만, 토큰당 활성화되는 전문가는 8개입니다. 전문가 하나의 크기를 줄이고 개수를 늘림으로써, 각 전문가가 특정 문법, 특정 코딩 패턴 등 매우 좁고 깊은 지식에 특화될 수 있게 만들었습니다.14전략 2: 공유 전문가 격리 (Shared Expert Isolation)이는 딥시크 아키텍처의 가장 독창적인 부분입니다. 기존 MoE에서는 모든 전문가가 라우팅(선택) 대상이었습니다. 그러나 딥시크 팀은 '관사(a, the)'나 기본적인 문장 구조와 같은 '범용 지식'은 모든 토큰에서 필요하다는 점에 주목했습니다. 이를 희소 전문가들이 중복해서 학습하는 것은 '지식의 중복(Knowledge Redundancy)'입니다.구현: 딥시크는 특정 전문가 그룹(Shared Experts)을 지정하여, 이들은 게이팅 네트워크의 선택과 상관없이 항상 활성화되도록 설계했습니다.수식적 표현: $y = \sum_{i \in Shared} E_i(x) + \sum_{j \in TopK(x)} g_j(x)E_j(x)$이 구조 덕분에 라우팅된 전문가들은 범용 지식을 학습할 필요 없이 고유한 전문 지식 학습에만 집중할 수 있게 되었습니다.154.3 하드웨어 제약 극복: MLA와 GRPO량원펑의 팀은 엔비디아의 최신 H100을 자유롭게 구할 수 없는 상황에서 H800(대역폭 축소판)을 사용해야 했습니다. 이를 극복하기 위해 두 가지 추가적인 혁신을 도입했습니다.MLA (Multi-head Latent Attention): 추론 시 KV 캐시(Key-Value Cache)가 차지하는 메모리를 획기적으로 줄여, 적은 수의 GPU로도 긴 문맥(Context)을 처리할 수 있게 한 어텐션 구조입니다.17GRPO (Group Relative Policy Optimization): DeepSeek-R1(추론 모델) 학습 시, 기존 PPO(Proximal Policy Optimization) 방식이 필요로 하는 '비평가(Critic) 모델'을 제거했습니다. 비평가 모델은 메모리를 두 배로 소모하는데, GRPO는 그룹 단위의 출력 평균을 기준선(Baseline)으로 삼아 비평가 없이도 강화학습을 가능케 하여 학습 효율을 극대화했습니다.195. 오픈소스 MoE 생태계: 라마(Llama) 너머의 세계사용자는 "Llama 외 소스가 공개된 사례"를 면밀히 찾아달라고 요청했습니다. 여기서 중요한 점은 Llama 시리즈(1, 2, 3)는 MoE가 아닌 밀집(Dense) 모델이라는 사실입니다(Llama 4부터 MoE 도입이 예상됨). 진정한 오픈소스 MoE의 계보는 프랑스의 미스트랄과 중국의 딥시크/알리바바가 주도하고 있습니다.5.1 미스트랄(Mistral) AI: 서구권 오픈 MoE의 표준프랑스의 스타트업 미스트랄 AI는 라마의 유출 사태 이후 등장하여, 오픈소스 MoE의 실질적인 표준을 제시했습니다.Mixtral 8x7B (2023년 12월 공개):최초의 고성능 오픈 가중치(Open Weights) MoE 모델.총 파라미터 47B 중 토큰당 13B만 활성화.의의: 라마 2 70B(밀집 모델)보다 성능은 뛰어나면서 추론 속도는 6배 빠름을 증명하여, 전 세계 개발자들에게 MoE의 효용성을 각인시켰습니다.15라이선스: Apache 2.0 (완전한 오픈소스).5.2 알리바바 Qwen-MoE와 xAI GrokQwen1.5-MoE (중국, 알리바바): 딥시크와 유사하게 미세 입자 전문가(Fine-grained) 방식을 채택했습니다. 딥시크의 가장 강력한 국내 경쟁자입니다.Grok-1 (미국, xAI): 일론 머스크의 xAI가 공개한 3140억 파라미터 규모의 초대형 MoE입니다. 구조적으로는 구글의 2017년 GShard 방식과 유사하며, 덩치가 너무 커서 일반 사용자가 활용하기 어렵다는 한계가 있습니다.5.3 라마(Llama)의 역할: 오픈소스의 기폭제라마 자체는 MoE가 아니었지만, 2023년 3월 4chan을 통한 가중치 유출 사건은 AI 연구의 흐름을 완전히 바꿨습니다. 메타(Meta)가 연구용으로만 제한적으로 배포했던 모델이 토렌트로 풀리면서, 딥시크를 포함한 전 세계 연구자들이 최신 LLM의 내부를 뜯어보고(Reverse Engineering), 그 위에 MoE 같은 새로운 아키텍처를 실험할 수 있는 토양을 제공했습니다.226. 프랑스와 인도 연구자들의 지정학적 역할사용자는 "프랑스, 인도 쪽 연구자"를 특정하여 살펴볼 것을 요청했습니다. 이들은 직접적으로 딥시크에 고용된 것은 아니나, 딥시크가 참조한 기술적 기반을 닦은 핵심 인물들입니다.6.1 프랑스: 오픈소스 저항군 (The French Resistance)프랑스 파리는 현재 미국 실리콘밸리에 대항하는 오픈소스 AI의 수도입니다. 이 생태계는 메타의 FAIR 파리 연구소와 구글 딥마인드 파리 출신들이 주도하고 있습니다.Guillaume Lample (기욤 램플):전 메타 FAIR 연구원, 현 미스트랄 AI 수석 과학자(Chief Scientist).Llama 1 논문의 핵심 저자 중 한 명입니다. 그는 메타 내부에서 거대 모델의 효율적 학습을 연구하다가, 회사의 폐쇄적 정책(또는 느린 오픈소스 전환)에 반기를 들고 미스트랄을 창업했습니다.24딥시크가 참고한 '효율적 MoE 학습'의 많은 노하우가 그의 연구(Mixtral)에서 검증되었습니다.Arthur Mensch (아르튀르 멘슈):전 구글 딥마인드 파리 연구원, 현 미스트랄 AI CEO.구글의 '친칠라(Chinchilla)' 스케일링 법칙 연구에 참여했습니다. 이는 "모델 크기보다 학습 데이터 양이 더 중요하다"는 이론으로, 딥시크가 상대적으로 작은 모델에 14.8조 토큰이라는 방대한 데이터를 쏟아붓는 전략의 이론적 근거가 되었습니다.266.2 인도: 트랜스포머의 설계자들 (Architects of the Transformer)인도 출신 연구자들은 딥시크 팀 내부에 직접 포진해 있지는 않으나, 딥시크 모델의 근간인 트랜스포머(Transformer) 아키텍처를 창시했습니다.Ashish Vaswani (아시시 바스와니):2017년 구글의 "Attention Is All You Need" 논문의 제1저자.현재 Essential AI를 창업하여 엔터프라이즈 AI 시장을 공략 중입니다.27중국과의 연결고리: 직접적인 인적 교류보다는 '지적 연결'이 강합니다. 중국의 모든 LLM 논문은 바스와니의 논문을 인용하며 시작합니다. 또한 마이크로소프트 리서치(MSR) 네트워크를 통해, MSR 인도(방갈로르)와 MSR 아시아(베이징) 간의 연구 교류가 간접적인 영향을 미쳤을 수 있습니다.7. 중국과의 연결고리: 마이크로소프트 리서치 아시아 (MSRA)사용자는 "중국 쪽과 연결고리"를 확인할 것을 요청했습니다. 조사 결과, 가장 강력하고 직접적인 연결고리는 **마이크로소프트 리서치 아시아(MSRA)**입니다. 이곳은 '중국 AI의 황포군관학교'로 불리며, 딥시크 핵심 인재들의 산실입니다.7.1 딥시크 기술팀의 DNA: MSRA 출신들딥시크의 기술 보고서와 논문 저자들을 추적해보면, 핵심 인물들이 MSRA 출신임이 명확히 드러납니다.이름역할 및 기여MSRA 관련 이력Daya Guo (郭达雅, 궈다야)딥시크 수석 연구원. DeepSeek-Coder, V3, R1 개발 주도.중산대학(Sun Yat-sen Univ.) 박사 과정 중 MSRA에서 3년 이상 연구 인턴 수행. 멘토는 MSRA의 자연어 처리 그룹 리더인 Nan Duan과 Ming Zhou였습니다. 그의 코딩 AI 연구(CodeBERT 등)는 MSRA 시절의 성과가 딥시크로 이어진 것입니다. 29Dejian Yang (양더젠)딥시크 핵심 기여자 (강화학습, 정렬)역시 MSRA 인턴 출신으로, 마이크로소프트의 글로벌 연구 네트워크 내에서 훈련받았습니다. 31Qihao Zhu (주치하오)딥시크 연구원 (코드 생성)베이징대 출신으로, MSRA 부원장 출신인 Zhou Ming 박사의 지도를 받는 등 학문적 계보가 MSRA와 얽혀 있습니다. 17인사이트: 딥시크의 기술력이 갑자기 튀어나온 것이 아닙니다. 지난 20년간 마이크로소프트가 베이징에 설립한 연구소가 키워낸 최정예 인재들이, 량원펑이라는 자본가를 만나 미중 기술 패권 경쟁의 최전선에 서게 된 것입니다. 이들은 미국식 연구 방법론(개방적 논문 출판, 영어 논문 작성, 벤치마크 중시)을 완벽하게 체득하고 있어, 딥시크의 기술 보고서가 서구권 연구자들에게 이질감 없이 받아들여지는 것입니다.8. 종합 결론 및 전망전문가 혼합(MoE) 아키텍처를 둘러싼 역사는 단순한 기술 발전을 넘어선 거대한 흐름을 보여줍니다.기원: 1991년 힌튼(Hinton)과 조던(Jordan) 등이 뇌의 분업화를 모방하며 씨앗을 뿌렸습니다.부활: 2017년 **구글(Shazeer, Hinton)**이 이를 GPU 시대에 맞게 '희소 게이팅'으로 부활시켰습니다.확산: 2023년 **프랑스(Mistral)**가 라마 사태의 파도를 타고 이를 오픈소스로 해방시켰습니다.진화: 2024년 **중국(DeepSeek/Liang Wenfeng)**이 미국의 제재라는 압박 속에서 '금융 공학적 효율성'과 'MSRA의 인재 풀'을 결합하여, 기존 모델을 능가하는 DeepSeekMoE로 진화시켰습니다.량원펑의 구현은 '미세 입자 전문가'와 '공유 전문가 격리'라는 독창적인 설계를 통해, 적은 메모리와 낮은 대역폭으로도 GPT-4급 성능을 내는 '비대칭 전력'을 확보하는 데 성공했습니다. 이는 AI 개발의 중심축이 '무제한 자원을 쏟아붓는 미국식 연구'에서 '제약 조건을 알고리즘으로 극복하는 효율성 중심의 연구'로 이동하고 있음을 시사합니다.결국 MoE는 토론토에서 태어나, 캘리포니아에서 자랐으며, 파리에서 자유를 얻고, 항저우에서 무기가 되었습니다. 이 글로벌한 지식의 파이프라인은 그 어떤 국경 장벽으로도 완전히 차단할 수 없음을 역사가 증명하고 있습니다.[부록] 주요 참조 데이터 및 출처MoE 기원 논문: Jacobs et al., 1991.1Google 2017 MoE: Shazeer et al., 2017.6DeepSeek 기술 보고서: DeepSeek-V3/R1 Technical Report.32Mistral/Llama 관련: Llama 1 Paper, Mistral AI Website.21MSRA 및 인재 정보: Daya Guo Profile, MSRA Alumni Lists.17