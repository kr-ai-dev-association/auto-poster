<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>차세대 대규모 언어 모델의 추론 메커니즘 심층 분석</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.tailwindcss.com?plugins=typography"></script>
    <style>
        mjx-container {
            display: inline !important;
            margin: 0 !important;
            vertical-align: middle;
            white-space: nowrap !important;
        }
        mjx-container[display="true"] {
            display: block !important;
            margin: 1.5em 0 !important;
            text-align: center;
            white-space: normal !important;
        }
        /* Prevent Tailwind prose from breaking math */
        .prose mjx-container {
            display: inline-block !important;
        }
        /* Force text color to black and code block styles */
        .wiki-html-content {
            color: #000 !important;
        }
        .wiki-html-content pre {
            background-color: #000 !important;
            color: #fff !important;
            padding: 1.5em !important;
            border-radius: 0.5rem !important;
            overflow-x: auto !important;
        }
        .wiki-html-content code {
            background-color: #000 !important;
            color: #fff !important;
            padding: 0.2em 0.4em !important;
            border-radius: 0.25rem !important;
            font-size: 0.9em !important;
        }
        /* Keep inline code blocks from looking weird inside paragraphs */
        p code, li code {
            display: inline !important;
            vertical-align: baseline !important;
        }
        /* Table styling */
        .wiki-html-content table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5em 0;
        }
        .wiki-html-content th, .wiki-html-content td {
            border: 1px solid #a2a9b1;
            padding: 8px;
            text-align: left;
        }
        .wiki-html-content th {
            background-color: #f8f9fa;
        }
    </style>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                packages: {'[+]': ['base', 'ams', 'noerrors', 'noundefined']}
            },
            svg: { fontCache: 'global', scale: 1.0 },
            startup: { typeset: true }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" id="MathJax-script" async></script>
</head>
<body class="bg-gray-50 py-10">
    <div class="max-w-4xl mx-auto bg-white p-8 shadow-md border border-gray-200">
        <article class="wiki-content">
            <div class="flex justify-between items-start border-b border-[#a2a9b1] pb-2 mb-6">
                <h1 class="text-3xl font-sans font-bold text-[#000] leading-tight">차세대 대규모 언어 모델의 추론 메커니즘 심층 분석: 아키텍처별 토큰 생성 동역학 및 엔지니어링 최적화 연구</h1>
                <div class="flex items-center gap-2 mt-2 ml-4 shrink-0">
                    <button class="p-1.5 text-gray-500 hover:text-blue-600 hover:bg-gray-100 rounded-full transition-all" title="Copy Link">
                        <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                    </button>
                </div>
            </div>

            <div class="my-6 rounded-lg overflow-hidden border border-[#a2a9b1] shadow-sm">
                <img src="../images/deep-analysis-next-gen-llm-reasoning_summary.png" alt="Summary Image" class="w-full h-auto object-cover" style="aspect-ratio: 16/9;">
            </div>

            <div class="wiki-html-content prose prose-slate max-w-none text-[#202122] leading-relaxed">
                <style>
                    .wiki-html-content @import url(https://themes.googleusercontent.com/fonts/css?kit=fz37I5dLqkukZAqqEcBXiiZegz6RfUqk5lRQgVKFIc16i_wyt05ZDg82Y47j6MDVmJ5_RyB2JGt_rxfp1qClyw);.lst-kix_3feobs7mewfs-0 > li:before{content:"" counter(lst-ctn-kix_3feobs7mewfs-0,.wiki-html-content decimal) ". "}.lst-kix_3feobs7mewfs-1 > li:before{.wiki-html-content content:"○ "}ul.lst-kix_list_1-0{.wiki-html-content list-style-type:none}ol.lst-kix_list_3-0{.wiki-html-content list-style-type:none}.lst-kix_3feobs7mewfs-4 > li:before{.wiki-html-content content:"■ "}.lst-kix_3feobs7mewfs-0 > li{.wiki-html-content counter-increment:lst-ctn-kix_3feobs7mewfs-0}.lst-kix_3feobs7mewfs-2 > li:before{.wiki-html-content content:"■ "}.lst-kix_3feobs7mewfs-3 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_3-0 > li:before{content:"" counter(lst-ctn-kix_list_3-0,.wiki-html-content decimal) ". "}ul.lst-kix_list_5-7{.wiki-html-content list-style-type:none}ul.lst-kix_list_5-8{.wiki-html-content list-style-type:none}.lst-kix_list_3-1 > li:before{.wiki-html-content content:"○ "}.lst-kix_list_3-2 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_5-5{.wiki-html-content list-style-type:none}ul.lst-kix_list_5-6{.wiki-html-content list-style-type:none}.lst-kix_list_8-1 > li:before{.wiki-html-content content:"○ "}.lst-kix_list_8-2 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_8-0 > li{.wiki-html-content counter-increment:lst-ctn-kix_list_8-0}ul.lst-kix_list_1-3{.wiki-html-content list-style-type:none}.lst-kix_list_3-5 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_5-0{.wiki-html-content list-style-type:none}ul.lst-kix_list_1-4{.wiki-html-content list-style-type:none}ul.lst-kix_list_1-1{.wiki-html-content list-style-type:none}.lst-kix_list_3-4 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_1-2{.wiki-html-content list-style-type:none}ul.lst-kix_list_5-3{.wiki-html-content list-style-type:none}ul.lst-kix_list_1-7{.wiki-html-content list-style-type:none}.lst-kix_list_3-3 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_5-4{.wiki-html-content list-style-type:none}ul.lst-kix_list_1-8{.wiki-html-content list-style-type:none}ul.lst-kix_list_5-1{.wiki-html-content list-style-type:none}.lst-kix_list_8-0 > li:before{content:"" counter(lst-ctn-kix_list_8-0,.wiki-html-content decimal) ". "}ul.lst-kix_list_1-5{.wiki-html-content list-style-type:none}ul.lst-kix_list_5-2{.wiki-html-content list-style-type:none}ul.lst-kix_list_1-6{.wiki-html-content list-style-type:none}.lst-kix_list_8-7 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_3-8 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_8-5 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_8-6 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_8-3 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_3-6 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_3-7 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_8-4 > li:before{.wiki-html-content content:"■ "}.lst-kix_3feobs7mewfs-8 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_3feobs7mewfs-7{.wiki-html-content list-style-type:none}ul.lst-kix_3feobs7mewfs-6{.wiki-html-content list-style-type:none}ul.lst-kix_3feobs7mewfs-8{.wiki-html-content list-style-type:none}ul.lst-kix_3feobs7mewfs-3{.wiki-html-content list-style-type:none}ul.lst-kix_3feobs7mewfs-2{.wiki-html-content list-style-type:none}.lst-kix_3feobs7mewfs-5 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_3feobs7mewfs-5{.wiki-html-content list-style-type:none}ul.lst-kix_3feobs7mewfs-4{.wiki-html-content list-style-type:none}.lst-kix_3feobs7mewfs-6 > li:before{.wiki-html-content content:"■ "}.lst-kix_3feobs7mewfs-7 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_8-8 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_3feobs7mewfs-1{.wiki-html-content list-style-type:none}.lst-kix_list_5-0 > li:before{.wiki-html-content content:"● "}.lst-kix_list_4-8 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_5-3 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_4-7 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_5-2 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_5-1 > li:before{.wiki-html-content content:"○ "}ul.lst-kix_list_4-8{.wiki-html-content list-style-type:none}.lst-kix_list_5-7 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_8-4{.wiki-html-content list-style-type:none}ul.lst-kix_list_8-5{.wiki-html-content list-style-type:none}ul.lst-kix_list_4-6{.wiki-html-content list-style-type:none}.lst-kix_list_5-6 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_5-8 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_8-2{.wiki-html-content list-style-type:none}ul.lst-kix_list_4-7{.wiki-html-content list-style-type:none}ul.lst-kix_list_8-3{.wiki-html-content list-style-type:none}ul.lst-kix_list_8-8{.wiki-html-content list-style-type:none}ul.lst-kix_list_8-6{.wiki-html-content list-style-type:none}ul.lst-kix_list_8-7{.wiki-html-content list-style-type:none}ul.lst-kix_list_4-0{.wiki-html-content list-style-type:none}ul.lst-kix_list_4-1{.wiki-html-content list-style-type:none}.lst-kix_list_5-4 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_4-4{.wiki-html-content list-style-type:none}.lst-kix_list_5-5 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_4-5{.wiki-html-content list-style-type:none}ul.lst-kix_list_8-1{.wiki-html-content list-style-type:none}ul.lst-kix_list_4-2{.wiki-html-content list-style-type:none}ul.lst-kix_list_4-3{.wiki-html-content list-style-type:none}.lst-kix_list_6-1 > li:before{.wiki-html-content content:"○ "}.lst-kix_list_6-3 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_6-0 > li:before{.wiki-html-content content:"● "}.lst-kix_list_6-4 > li:before{.wiki-html-content content:"■ "}ol.lst-kix_3feobs7mewfs-0{.wiki-html-content list-style-type:none}.lst-kix_list_3-0 > li{.wiki-html-content counter-increment:lst-ctn-kix_list_3-0}.lst-kix_list_6-2 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_6-8 > li:before{.wiki-html-content content:"■ "}ol.lst-kix_list_8-0.start{.wiki-html-content counter-reset:lst-ctn-kix_list_8-0 0}.lst-kix_list_6-5 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_6-7 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_7-0 > li:before{.wiki-html-content content:"● "}.lst-kix_list_6-6 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_2-6 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_2-7 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_7-4 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_7-6 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_2-4 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_2-5 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_2-8 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_7-1 > li:before{.wiki-html-content content:"○ "}.lst-kix_list_7-5 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_7-2 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_7-3 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_7-5{.wiki-html-content list-style-type:none}ul.lst-kix_list_7-6{.wiki-html-content list-style-type:none}ul.lst-kix_list_7-3{.wiki-html-content list-style-type:none}ul.lst-kix_list_3-7{.wiki-html-content list-style-type:none}ul.lst-kix_list_7-4{.wiki-html-content list-style-type:none}ul.lst-kix_list_3-8{.wiki-html-content list-style-type:none}ul.lst-kix_list_7-7{.wiki-html-content list-style-type:none}ul.lst-kix_list_7-8{.wiki-html-content list-style-type:none}ol.lst-kix_list_3-0.start{.wiki-html-content counter-reset:lst-ctn-kix_list_3-0 0}ul.lst-kix_list_3-1{.wiki-html-content list-style-type:none}ul.lst-kix_list_3-2{.wiki-html-content list-style-type:none}.lst-kix_list_7-8 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_7-1{.wiki-html-content list-style-type:none}ul.lst-kix_list_3-5{.wiki-html-content list-style-type:none}ul.lst-kix_list_7-2{.wiki-html-content list-style-type:none}ul.lst-kix_list_3-6{.wiki-html-content list-style-type:none}ul.lst-kix_list_3-3{.wiki-html-content list-style-type:none}ul.lst-kix_list_7-0{.wiki-html-content list-style-type:none}.lst-kix_list_7-7 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_3-4{.wiki-html-content list-style-type:none}.lst-kix_list_4-0 > li:before{.wiki-html-content content:"● "}.lst-kix_list_4-1 > li:before{.wiki-html-content content:"○ "}.lst-kix_list_4-4 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_4-3 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_4-5 > li:before{.wiki-html-content content:"■ "}ol.lst-kix_3feobs7mewfs-0.start{.wiki-html-content counter-reset:lst-ctn-kix_3feobs7mewfs-0 0}.lst-kix_list_4-2 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_4-6 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_6-6{.wiki-html-content list-style-type:none}ul.lst-kix_list_6-7{.wiki-html-content list-style-type:none}ul.lst-kix_list_6-4{.wiki-html-content list-style-type:none}ul.lst-kix_list_2-8{.wiki-html-content list-style-type:none}ul.lst-kix_list_6-5{.wiki-html-content list-style-type:none}ul.lst-kix_list_6-8{.wiki-html-content list-style-type:none}ul.lst-kix_list_2-2{.wiki-html-content list-style-type:none}.lst-kix_list_1-0 > li:before{.wiki-html-content content:"● "}ul.lst-kix_list_2-3{.wiki-html-content list-style-type:none}ul.lst-kix_list_2-0{.wiki-html-content list-style-type:none}ul.lst-kix_list_2-1{.wiki-html-content list-style-type:none}ul.lst-kix_list_6-2{.wiki-html-content list-style-type:none}ol.lst-kix_list_8-0{.wiki-html-content list-style-type:none}ul.lst-kix_list_2-6{.wiki-html-content list-style-type:none}ul.lst-kix_list_6-3{.wiki-html-content list-style-type:none}.lst-kix_list_1-1 > li:before{.wiki-html-content content:"○ "}.lst-kix_list_1-2 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_2-7{.wiki-html-content list-style-type:none}ul.lst-kix_list_6-0{.wiki-html-content list-style-type:none}ul.lst-kix_list_2-4{.wiki-html-content list-style-type:none}ul.lst-kix_list_6-1{.wiki-html-content list-style-type:none}ul.lst-kix_list_2-5{.wiki-html-content list-style-type:none}.lst-kix_list_1-3 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_1-4 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_1-7 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_1-5 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_1-6 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_2-0 > li:before{.wiki-html-content content:"● "}.lst-kix_list_2-1 > li:before{.wiki-html-content content:"○ "}.lst-kix_list_1-8 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_2-2 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_2-3 > li:before{content:"■ "}
                </style>
                
                <h2>1. 서론: LLM 추론의 패러다임 전환과 엔지니어링 난제</h2>
                <p>2024년과 2025년은 인공지능, 특히 대규모 언어 모델(Large Language Model, LLM)의 역사에 있어 중요한 변곡점으로 기록될 것입니다. GPT-3의 등장 이후 2023년까지의 연구가 주로 모델의 파라미터 크기를 키우고 학습 데이터의 양을 늘려 '창발적 능력(Emergent Capabilities)'을 확보하는 '스케일링 법칙(Scaling Law)'의 시대였다면, 2024년 이후는 확보된 지능을 실제 하드웨어 제약 내에서 효율적으로 서빙(Serving)하고 추론(Inference)하는 '효율성(Efficiency)과 아키텍처(Architecture)'의 시대로 정의할 수 있습니다.</p>
                <p>본 보고서는 2024년 이후 등장한 세 가지의 상징적인 모델 아키텍처—Meta의 Llama 3.1 (Dense Transformer), DeepSeek-AI의 DeepSeek-V3 (MoE + MLA), AI21 Labs의 Jamba 1.5 (Hybrid SSM-Transformer)—를 대상으로, 각 모델이 토큰을 생성할 때 내부적으로 작동하는 추론 메커니즘을 심층적으로 분석합니다.</p>
                <p>현대 LLM 추론이 직면한 핵심적인 공학적 난제는 '메모리 장벽(Memory Wall)'입니다. 오토리그레시브(Autoregressive) 생성 과정에서 모델은 이전에 생성된 모든 토큰의 키(Key)와 밸류(Value) 상태를 저장하는 KV 캐시(KV Cache)를 유지해야 합니다. 컨텍스트 길이가 128K 토큰 이상으로 확장됨에 따라, 이 KV 캐시의 크기는 기하급수적으로 증가하여 GPU의 고대역폭 메모리(HBM)를 소진시키고, 결과적으로 시스템의 전체 처리량(Throughput)을 제한하는 주요 병목이 되었습니다.</p>
                <p>우리는 이 보고서에서 각 모델이 이러한 물리적 한계를 극복하기 위해 어떠한 수학적 기법과 알고리즘적 혁신을 도입했는지 분석합니다. Llama 3.1이 선택한 FP8 양자화와 거대 밀집(Dense) 모델의 최적화 전략, DeepSeek-V3가 제안한 다중 헤드 잠재 어텐션(Multi-head Latent Attention, MLA)을 통한 KV 캐시 압축의 수학적 원리, 그리고 Jamba 1.5가 시도한 Mamba(상태 공간 모델)와 Transformer의 하이브리드 결합이 가져온 선형적 확장성을 비교합니다. 또한, 실제 커뮤니티와 벤치마크에서 관찰된 각 아키텍처의 한계와 오류 사례를 통해 이론과 실제의 간극을 조명합니다.</p>

                <h2>2. LLM 추론의 이론적 배경과 핵심 병목 현상</h2>
                <p>각 모델의 구체적인 메커니즘을 분석하기에 앞서, 현대 LLM 추론 시스템이 해결해야 할 근원적인 계산 및 메모리 역학을 이해할 필요가 있습니다. LLM의 추론은 크게 두 단계, 즉 입력 프롬프트를 처리하는 프리필(Prefill) 단계와 토큰을 하나씩 생성하는 디코딩(Decoding) 단계로 나뉩니다.</p>

                <h3>2.1 오토리그레시브 디코딩과 메모리 대역폭의 제약</h3>
                <p>Transformer 기반 모델의 디코딩 단계는 본질적으로 메모리 대역폭 의존적(Memory-bound)인 작업입니다. 모델이 시점 $t$에서 새로운 토큰 $x_t$를 생성하기 위해서는 모델의 모든 가중치(Parameters)를 GPU 메모리에서 연산 유닛(Tensor Core 등)으로 이동시켜야 합니다. 배치 크기가 작을 때, GPU의 연산 능력(FLOPS)은 남아돌지만 데이터를 메모리에서 가져오는 속도가 이를 따라가지 못해 지연 시간이 발생합니다.</p>
                <p>수학적으로, $P$개의 파라미터를 가진 모델이 $B$의 배치 크기로 추론을 수행할 때, 각 토큰 생성 단계에서의 연산 집약도(Arithmetic Intensity)는 매우 낮습니다. 이를 극복하기 위해 배치 크기를 키워야 하지만, 배치 크기를 키우면 각 시퀀스의 KV 캐시가 메모리를 점유하여 최대 배치 크기를 제한하게 됩니다.</p>

                <h3>2.2 KV 캐시(KV Cache)의 폭발적 증가</h3>
                <p>어텐션 메커니즘은 쿼리($Q$), 키($K$), 밸류($V$)의 상호작용으로 이루어집니다.</p>
                <p>$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$</p>
                <p>여기서 $Q$는 현재 생성하려는 토큰에 대한 벡터이지만, $K$와 $V$는 과거의 모든 토큰에 대한 정보를 담고 있어야 합니다. 매 스텝마다 과거 토큰들의 $K, V$를 다시 계산하는 것은 비효율적이므로 이를 VRAM에 캐싱합니다.</p>
                <p>Llama 2 시대까지만 해도 4K 수준이었던 컨텍스트 윈도우가 Llama 3.1과 Jamba 1.5에 이르러 128K~256K로 확장되면서, KV 캐시의 크기는 모델 가중치 자체보다 더 큰 메모리를 요구하게 되었습니다. 예를 들어, FP16 정밀도에서 128K 토큰을 저장할 경우 수백 기가바이트의 메모리가 필요할 수 있으며, 이는 단일 GPU에서 처리 불가능한 수준입니다.</p>

                <h3>2.3 어텐션 메커니즘의 진화: MHA에서 MLA까지</h3>
                <table>
                    <thead>
                        <tr>
                            <th>아키텍처</th>
                            <th>설명</th>
                            <th>메모리 효율</th>
                            <th>대표 모델</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>MHA (Multi-Head Attention)</td>
                            <td>모든 헤드가 고유한 KV를 가짐.</td>
                            <td>낮음 (최대 메모리 사용)</td>
                            <td>GPT-3, Llama 1</td>
                        </tr>
                        <tr>
                            <td>MQA (Multi-Query Attention)</td>
                            <td>모든 헤드가 하나의 KV를 공유.</td>
                            <td>매우 높음 (성능 저하 위험)</td>
                            <td>Falcon, Gemini 1.0</td>
                        </tr>
                        <tr>
                            <td>GQA (Grouped-Query Attention)</td>
                            <td>헤드를 그룹으로 묶어 KV를 공유.</td>
                            <td>중간 (성능과 효율의 균형)</td>
                            <td>Llama 2, Llama 3.1</td>
                        </tr>
                        <tr>
                            <td>MLA (Multi-Head Latent Attention)</td>
                            <td>KV를 저랭크 잠재 벡터로 압축.</td>
                            <td>매우 높음 (성능 유지)</td>
                            <td>DeepSeek-V2, V3</td>
                        </tr>
                    </tbody>
                </table>

                <h2>3. Llama 3.1 405B: 고밀도(Dense) 트랜스포머의 극한과 엔지니어링</h2>
                <p>Meta가 2024년 7월 공개한 Llama 3.1 405B는 오픈 웨이트(Open Weights) 모델 중 가장 거대한 규모를 자랑합니다. 이 모델은 아키텍처적으로는 기존의 Transformer 구조를 계승하고 있으나, 4050억 개라는 파라미터 수와 128K라는 컨텍스트 길이는 추론 시스템에 극한의 부하를 줍니다.</p>

                <h3>3.1 추론 아키텍처와 GQA (Grouped-Query Attention)</h3>
                <p>Llama 3.1 405B는 표준적인 Decoder-only Transformer 구조를 기반으로 하며, 추론 효율성을 위해 <strong>GQA (Grouped-Query Attention)</strong>를 채택했습니다.</p>
                <ul>
                    <li><strong>메커니즘:</strong> 전체 쿼리 헤드(Query Heads)를 $G$개의 그룹으로 나누고, 각 그룹 내의 쿼리 헤드들은 하나의 KV 헤드(Key-Value Head)를 공유합니다.</li>
                    <li><strong>효과:</strong> 이를 통해 KV 캐시의 크기를 $G$배만큼 줄일 수 있습니다. Llama 3.1 405B의 경우, GQA를 통해 128K 컨텍스트에서도 KV 캐시 메모리 요구량을 관리 가능한 수준으로 억제하려 시도했습니다. 하지만 405B라는 모델 자체의 크기 때문에, FP16 정밀도로 모델을 로드하는 것만으로도 약 810GB의 VRAM이 필요하며, 이는 80GB H100 GPU 8장을 연결한 단일 노드(640GB) 용량을 초과합니다.</li>
                </ul>

                <h3>3.2 FP8 양자화: 물리적 한계의 극복</h3>
                <p>Llama 3.1 405B를 실질적으로 서빙하기 위해서는 FP8 (8-bit Floating Point) 양자화가 필수적입니다. Meta와 커뮤니티는 FP8 양자화를 통해 메모리 사용량을 절반으로 줄여 8xH100 노드 하나에 모델을 적재하는 방식을 표준으로 채택하고 있습니다.</p>

                <h4>3.2.1 FP8 추론의 수학적 원리</h4>
                <p>FP8은 E4M3(4비트 지수, 3비트 가수) 또는 E5M2(5비트 지수, 2비트 가수) 형식을 사용합니다. 추론 시에는 다음과 같은 과정이 일어납니다:</p>
                <ul>
                    <li><strong>가중치 스케일링:</strong> 각 레이어의 가중치 텐서를 FP8로 변환하며, 이때 다이나믹 레인지를 보존하기 위해 스케일링 팩터(Scaling Factor)를 계산합니다.</li>
                    <li><strong>활성화 양자화:</strong> 입력 토큰의 활성화(Activation) 값 또한 실시간으로 FP8로 변환됩니다.</li>
                    <li><strong>GEMM 연산:</strong> H100의 Tensor Core는 두 FP8 행렬의 곱셈을 수행하고, 결과를 FP16 또는 FP32로 누적(Accumulation)하여 정밀도 손실을 최소화합니다.</li>
                </ul>

                <h3>3.3 코드 검증: generate.py 및 분산 추론 환경</h3>
                <pre><code class="language-python"># Llama 3.1 분산 추론 의사 코드 (PyTorch/FairScale 기반)
import torch.distributed as dist
from llama.model import Transformer

def main():
    # 프로세스 그룹 초기화 (NCCL 백엔드 사용)
    dist.init_process_group("nccl")
    rank = dist.get_rank()
    world_size = dist.get_world_size()

    # 모델 로드 (각 GPU는 모델의 일부인 'Shard'만 로드함)
    # Tensor Parallelism이 적용되어 각 레이어의 가중치가 분할됨
    model = Transformer(params).to(local_rank)
    
    # 생성 루프
    for cur_pos in range(seq_len):
        # 각 GPU에서 부분 연산 수행
        logits = model(tokens[:, cur_pos])
        # 연산 결과 동기화 (All-Reduce 통신 발생)
        dist.all_reduce(logits) </code></pre>

                <pre><code class="language-bash"># vLLM을 이용한 Llama 3.1 405B FP8 서빙 명령어 예시
vllm serve meta-llama/Meta-Llama-3.1-405B-Instruct-FP8 \
  --tensor-parallel-size 8 \
  --max-model-len 8192 \
  --kv-cache-dtype fp8</code></pre>

                <h2>4. DeepSeek-V3: 아키텍처적 압축과 희소성(Sparsity)의 혁신</h2>
                <p>DeepSeek-V3(671B)는 전체 파라미터 수가 Llama 3.1 405B보다 크지만, 추론 시에는 토큰당 37B 파라미터만 활성화하는 Mixture-of-Experts (MoE) 구조를 취합니다. 그러나 DeepSeek-V3의 진정한 혁신은 <strong>Multi-head Latent Attention (MLA)</strong>이라는 새로운 어텐션 메커니즘에 있습니다.</p>

                <h3>4.1 Multi-head Latent Attention (MLA): KV 캐시의 압축</h3>
                <p>MLA는 기존 MHA나 GQA와 달리, 키(Key)와 밸류(Value)를 그대로 캐싱하지 않고, 이를 저차원 잠재 벡터(Latent Vector)로 압축하여 저장합니다.</p>

                <h4>4.1.1 MLA의 수학적 구조와 '행렬 흡수(Matrix Absorption)'</h4>
                <p>MLA는 입력을 두 단계로 나눕니다:</p>
                <ul>
                    <li><strong>압축:</strong>$c_{KV} = W_{DKV} h_t$</li>
                    <li><strong>복원:</strong>$k = W_{UK} c_{KV}, \quad v = W_{UV} c_{KV}$</li>
                </ul>
                <p>어텐션 스코어 계산 식: $q^T k = (W_Q x)^T (W_{UK} c_{KV}) = x^T W_Q^T W_{UK} c_{KV}$</p>
                <p>이 식에서 $W_Q^T W_{UK}$ 부분을 미리 계산하여 하나의 행렬로 합칠 수 있다면(Absorb), 추론 시에는 $c_{KV}$를 복원할 필요 없이 압축된 상태 그대로 연산할 수 있습니다. 즉, KV 캐시에는 매우 작은 $c_{KV}$만 저장하면 됩니다.</p>

                <h4>4.1.2 Decoupled RoPE: 위치 정보의 분리</h4>
                <p>DeepSeek-V3는 위치 정보(RoPE)를 적용해야 하는 부분은 별도의 작은 차원(예: 64차원)으로 분리하여 압축하지 않고 그대로 캐시에 저장합니다. 결과적으로 KV 캐시 크기를 약 93.3%까지 줄일 수 있었습니다.</p>

                <pre><code class="language-python"># DeepSeek-V3 추론 코드 구조 분석
class MLA(nn.Module):
    def forward(self, x, start_pos, freqs_cis, mask):
        # 1. 압축된 잠재 벡터 생성
        c_kv = self.w_dkv(x) 
        
        # 2. Decoupled RoPE: RoPE 적용 부분만 따로 계산
        k_rope = apply_rope(self.w_uk_rope(x), freqs_cis)
        
        # 3. KV 캐시 저장 (복원된 거대 행렬을 저장하지 않음!)
        self.kv_cache.update(c_kv, k_rope, start_pos)
        
        # 4. 어텐션 연산 (FlashMLA 활용)</code></pre>

                <h2>5. Jamba 1.5: 하이브리드 아키텍처와 선형 확장성의 실현</h2>
                <p>AI21 Labs의 Jamba 1.5는 Transformer와 Mamba 레이어를 섞어 쓰는 하이브리드 구조를 통해 "무한에 가까운 문맥"을 효율적으로 처리합니다.</p>

                <h3>5.1 Mamba와 Transformer의 인터리빙</h3>
                <p>Jamba 1.5 Large (398B)는 8개의 레이어마다 1개의 Transformer 어텐션 레이어를 배치하고, 나머지 7개는 Mamba 레이어로 채웁니다.</p>
                <ul>
                    <li><strong>Mamba 레이어:</strong> 고정된 크기의 상태(State) 변수 $h_t$를 업데이트합니다. ($h_t = A h_{t-1} + B x_t$) 메모리와 연산량이 문맥 길이와 상관없이 $O(1)$입니다.</li>
                    <li><strong>Transformer 레이어:</strong> Mamba의 '망각' 문제를 보완하기 위해 전체 문맥을 명시적으로 참조(Attention)합니다. 전체 모델의 KV 캐시는 순수 Transformer의 1/8 수준입니다.</li>
                </ul>

                <pre><code class="language-python"># Jamba 1.5 추론 코드 예시
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "ai21labs/AI21-Jamba-1.5-Large",
    use_mamba_kernels=True,
    device_map="auto"
)</code></pre>

                <h2>6. 심화 분석: 스펙큘러티브 디코딩(Speculative Decoding)</h2>
                <ul>
                    <li><strong>Llama 3.1:</strong> 작은 모델(Llama 3.1 8B)을 드래프트 모델로 사용하여 약 2배 이상의 속도 향상을 얻습니다.</li>
                    <li><strong>DeepSeek-V3:</strong> <strong>MTP (Multi-Token Prediction)</strong>를 통해 한 번의 패스에서 여러 토큰을 예측, 내장된 스펙큘러티브 디코딩 효과를 냅니다.</li>
                </ul>

                <h2>7. 비교 분석 및 결론: 트릴레마의 해법</h2>
                <table>
                    <thead>
                        <tr>
                            <th>특징</th>
                            <th>Llama 3.1 405B</th>
                            <th>DeepSeek-V3 (671B)</th>
                            <th>Jamba 1.5 Large (398B)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>핵심 철학</td>
                            <td>Scale & Quality</td>
                            <td>Efficiency & Compression</td>
                            <td>Long Context & Hybrid</td>
                        </tr>
                        <tr>
                            <td>추론 아키텍처</td>
                            <td>Dense Transformer + GQA</td>
                            <td>MoE + MLA (KV 압축)</td>
                            <td>Hybrid (Mamba + Attn)</td>
                        </tr>
                        <tr>
                            <td>메모리 병목 해법</td>
                            <td>FP8 양자화</td>
                            <td>MLA (KV 캐시 93% 감소)</td>
                            <td>SSM State (KV 8배 감소)</td>
                        </tr>
                        <tr>
                            <td>장점</td>
                            <td>최고 성능, 넓은 생태계</td>
                            <td>압도적인 생성 비용 효율성</td>
                            <td>초장문 처리 최적화</td>
                        </tr>
                    </tbody>
                </table>

                <p><strong>결론:</strong> 2025년 현재, LLM 추론 기술은 단순한 '버티기'에서 '지능적인 압축'으로 이동하고 있습니다. 엔지니어와 연구자들은 이제 모델의 파라미터 수보다, 모델이 사용하는 <strong>'KV 캐시의 크기'</strong>와 <strong>'토큰 생성 당 메모리 대역폭 요구량'</strong>을 더 중요한 지표로 삼아야 할 것입니다.</p>
            </div>

            <div class="mt-12 pt-4 border-t border-[#a2a9b1] text-xs text-[#444] font-medium italic">
                This page was last edited on Dec 30, 2025.
            </div>
        </article>
    </div>
</body>
</html>