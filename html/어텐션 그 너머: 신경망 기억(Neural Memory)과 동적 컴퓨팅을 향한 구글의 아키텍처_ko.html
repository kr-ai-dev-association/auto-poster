<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>어텐션 그 너머: 신경망 기억(Neural Memory)과 동적 컴퓨팅을 향한 구글의 아키텍처 혁신</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.tailwindcss.com?plugins=typography"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                packages: {'[+]': ['base', 'ams', 'noerrors', 'noundefined']}
            },
            svg: { fontCache: 'global', scale: 1.0, displayAlign: 'center' },
            startup: { typeset: true }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" id="MathJax-script" async></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300;400;500;700&display=swap');
        body { font-family: 'Noto Sans KR', sans-serif; }
    </style>
</head>
<body class="bg-gray-50 py-10 px-4">
    <div class="max-w-4xl mx-auto bg-white p-8 shadow-sm border border-[#a2a9b1] rounded-sm">
        <article class="wiki-content">
            <div class="flex justify-between items-start border-b border-[#a2a9b1] pb-2 mb-6">
                <h1 class="text-3xl font-sans font-bold text-[#000] leading-tight">어텐션 그 너머: 신경망 기억(Neural Memory)과 동적 컴퓨팅을 향한 구글의 아키텍처 혁신</h1>
                <div class="flex items-center gap-2 mt-2 ml-4 shrink-0">
                    <button class="p-1.5 text-gray-500 hover:text-blue-600 hover:bg-gray-100 rounded-full transition-all" title="Copy Link">
                        <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                    </button>
                </div>
            </div>

            <div class="my-6 rounded-lg overflow-hidden border border-[#a2a9b1] shadow-sm">
                <img src="images/beyond-attention-google-neural-memory-dynamic-computing_summary.png" alt="Summary Image" class="w-full h-auto object-cover" style="aspect-ratio: 16/9;">
            </div>

            <div class="wiki-html-content prose prose-slate max-w-none text-[#202122] leading-relaxed">
                <style>
                    .wiki-html-content @import url(https://themes.googleusercontent.com/fonts/css?kit=fz37I5dLqkukZAqqEcBXiiZegz6RfUqk5lRQgVKFIc16i_wyt05ZDg82Y47j6MDVmJ5_RyB2JGt_rxfp1qClyw);.lst-kix_3feobs7mewfs-0 > li:before{content:"" counter(lst-ctn-kix_3feobs7mewfs-0,.wiki-html-content decimal) ". "}.lst-kix_3feobs7mewfs-1 > li:before{.wiki-html-content content:"○ "}ul.lst-kix_list_1-0{.wiki-html-content list-style-type:none}ol.lst-kix_list_3-0{.wiki-html-content list-style-type:none}.lst-kix_3feobs7mewfs-4 > li:before{.wiki-html-content content:"■ "}.lst-kix_3feobs7mewfs-0 > li{.wiki-html-content counter-increment:lst-ctn-kix_3feobs7mewfs-0}.lst-kix_3feobs7mewfs-2 > li:before{.wiki-html-content content:"■ "}.lst-kix_3feobs7mewfs-3 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_3-0 > li:before{content:"" counter(lst-ctn-kix_list_3-0,.wiki-html-content decimal) ". "}ul.lst-kix_list_5-7{.wiki-html-content list-style-type:none}ul.lst-kix_list_5-8{.wiki-html-content list-style-type:none}.lst-kix_list_3-1 > li:before{.wiki-html-content content:"○ "}.lst-kix_list_3-2 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_5-5{.wiki-html-content list-style-type:none}ul.lst-kix_list_5-6{.wiki-html-content list-style-type:none}.lst-kix_list_8-1 > li:before{.wiki-html-content content:"○ "}.lst-kix_list_8-2 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_8-0 > li{.wiki-html-content counter-increment:lst-ctn-kix_list_8-0}ul.lst-kix_list_1-3{.wiki-html-content list-style-type:none}.lst-kix_list_3-5 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_5-0{.wiki-html-content list-style-type:none}ul.lst-kix_list_1-4{.wiki-html-content list-style-type:none}ul.lst-kix_list_1-1{.wiki-html-content list-style-type:none}.lst-kix_list_3-4 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_1-2{.wiki-html-content list-style-type:none}ul.lst-kix_list_5-3{.wiki-html-content list-style-type:none}ul.lst-kix_list_1-7{.wiki-html-content list-style-type:none}.lst-kix_list_3-3 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_5-4{.wiki-html-content list-style-type:none}ul.lst-kix_list_1-8{.wiki-html-content list-style-type:none}ul.lst-kix_list_5-1{.wiki-html-content list-style-type:none}.lst-kix_list_8-0 > li:before{content:"" counter(lst-ctn-kix_list_8-0,.wiki-html-content decimal) ". "}ul.lst-kix_list_1-5{.wiki-html-content list-style-type:none}ul.lst-kix_list_5-2{.wiki-html-content list-style-type:none}ul.lst-kix_list_1-6{.wiki-html-content list-style-type:none}.lst-kix_list_8-7 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_3-8 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_8-5 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_8-6 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_8-3 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_3-6 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_3-7 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_8-4 > li:before{.wiki-html-content content:"■ "}.lst-kix_3feobs7mewfs-8 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_3feobs7mewfs-7{.wiki-html-content list-style-type:none}ul.lst-kix_3feobs7mewfs-6{.wiki-html-content list-style-type:none}ul.lst-kix_3feobs7mewfs-8{.wiki-html-content list-style-type:none}ul.lst-kix_3feobs7mewfs-3{.wiki-html-content list-style-type:none}ul.lst-kix_3feobs7mewfs-2{.wiki-html-content list-style-type:none}.lst-kix_3feobs7mewfs-5 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_3feobs7mewfs-5{.wiki-html-content list-style-type:none}ul.lst-kix_3feobs7mewfs-4{.wiki-html-content list-style-type:none}.lst-kix_3feobs7mewfs-6 > li:before{.wiki-html-content content:"■ "}.lst-kix_3feobs7mewfs-7 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_8-8 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_3feobs7mewfs-1{.wiki-html-content list-style-type:none}.lst-kix_list_5-0 > li:before{.wiki-html-content content:"● "}.lst-kix_list_4-8 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_5-3 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_4-7 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_5-2 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_5-1 > li:before{.wiki-html-content content:"○ "}ul.lst-kix_list_4-8{.wiki-html-content list-style-type:none}.lst-kix_list_5-7 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_8-4{.wiki-html-content list-style-type:none}ul.lst-kix_list_8-5{.wiki-html-content list-style-type:none}ul.lst-kix_list_4-6{.wiki-html-content list-style-type:none}.lst-kix_list_5-6 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_5-8 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_8-2{.wiki-html-content list-style-type:none}ul.lst-kix_list_4-7{.wiki-html-content list-style-type:none}ul.lst-kix_list_8-3{.wiki-html-content list-style-type:none}ul.lst-kix_list_8-8{.wiki-html-content list-style-type:none}ul.lst-kix_list_8-6{.wiki-html-content list-style-type:none}ul.lst-kix_list_8-7{.wiki-html-content list-style-type:none}ul.lst-kix_list_4-0{.wiki-html-content list-style-type:none}ul.lst-kix_list_4-1{.wiki-html-content list-style-type:none}.lst-kix_list_5-4 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_4-4{.wiki-html-content list-style-type:none}.lst-kix_list_5-5 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_4-5{.wiki-html-content list-style-type:none}ul.lst-kix_list_8-1{.wiki-html-content list-style-type:none}ul.lst-kix_list_4-2{.wiki-html-content list-style-type:none}ul.lst-kix_list_4-3{.wiki-html-content list-style-type:none}.lst-kix_list_6-1 > li:before{.wiki-html-content content:"○ "}.lst-kix_list_6-3 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_6-0 > li:before{.wiki-html-content content:"● "}.lst-kix_list_6-4 > li:before{.wiki-html-content content:"■ "}ol.lst-kix_3feobs7mewfs-0{.wiki-html-content list-style-type:none}.lst-kix_list_3-0 > li{.wiki-html-content counter-increment:lst-ctn-kix_list_3-0}.lst-kix_list_6-2 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_6-8 > li:before{.wiki-html-content content:"■ "}ol.lst-kix_list_8-0.start{.wiki-html-content counter-reset:lst-ctn-kix_list_8-0 0}.lst-kix_list_6-5 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_6-7 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_7-0 > li:before{.wiki-html-content content:"● "}.lst-kix_list_6-6 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_2-6 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_2-7 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_7-4 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_7-6 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_2-4 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_2-5 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_2-8 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_7-1 > li:before{.wiki-html-content content:"○ "}.lst-kix_list_7-5 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_7-2 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_7-3 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_7-5{.wiki-html-content list-style-type:none}ul.lst-kix_list_7-6{.wiki-html-content list-style-type:none}ul.lst-kix_list_7-3{.wiki-html-content list-style-type:none}ul.lst-kix_list_3-7{.wiki-html-content list-style-type:none}ul.lst-kix_list_7-4{.wiki-html-content list-style-type:none}ul.lst-kix_list_3-8{.wiki-html-content list-style-type:none}ul.lst-kix_list_7-7{.wiki-html-content list-style-type:none}ul.lst-kix_list_7-8{.wiki-html-content list-style-type:none}ol.lst-kix_list_3-0.start{.wiki-html-content counter-reset:lst-ctn-kix_list_3-0 0}ul.lst-kix_list_3-1{.wiki-html-content list-style-type:none}ul.lst-kix_list_3-2{.wiki-html-content list-style-type:none}.lst-kix_list_7-8 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_7-1{.wiki-html-content list-style-type:none}ul.lst-kix_list_3-5{.wiki-html-content list-style-type:none}ul.lst-kix_list_7-2{.wiki-html-content list-style-type:none}ul.lst-kix_list_3-6{.wiki-html-content list-style-type:none}ul.lst-kix_list_3-3{.wiki-html-content list-style-type:none}ul.lst-kix_list_7-0{.wiki-html-content list-style-type:none}.lst-kix_list_7-7 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_3-4{.wiki-html-content list-style-type:none}.lst-kix_list_4-0 > li:before{.wiki-html-content content:"● "}.lst-kix_list_4-1 > li:before{.wiki-html-content content:"○ "}.lst-kix_list_4-4 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_4-3 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_4-5 > li:before{.wiki-html-content content:"■ "}ol.lst-kix_3feobs7mewfs-0.start{.wiki-html-content counter-reset:lst-ctn-kix_3feobs7mewfs-0 0}.lst-kix_list_4-2 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_4-6 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_6-6{.wiki-html-content list-style-type:none}ul.lst-kix_list_6-7{.wiki-html-content list-style-type:none}ul.lst-kix_list_6-4{.wiki-html-content list-style-type:none}ul.lst-kix_list_2-8{.wiki-html-content list-style-type:none}ul.lst-kix_list_6-5{.wiki-html-content list-style-type:none}ul.lst-kix_list_6-8{.wiki-html-content list-style-type:none}ul.lst-kix_list_2-2{.wiki-html-content list-style-type:none}.lst-kix_list_1-0 > li:before{.wiki-html-content content:"● "}ul.lst-kix_list_2-3{.wiki-html-content list-style-type:none}ul.lst-kix_list_2-0{.wiki-html-content list-style-type:none}ul.lst-kix_list_2-1{.wiki-html-content list-style-type:none}ul.lst-kix_list_6-2{.wiki-html-content list-style-type:none}ol.lst-kix_list_8-0{.wiki-html-content list-style-type:none}ul.lst-kix_list_2-6{.wiki-html-content list-style-type:none}ul.lst-kix_list_6-3{.wiki-html-content list-style-type:none}.lst-kix_list_1-1 > li:before{.wiki-html-content content:"○ "}.lst-kix_list_1-2 > li:before{.wiki-html-content content:"■ "}ul.lst-kix_list_2-7{.wiki-html-content list-style-type:none}ul.lst-kix_list_6-0{.wiki-html-content list-style-type:none}ul.lst-kix_list_2-4{.wiki-html-content list-style-type:none}ul.lst-kix_list_6-1{.wiki-html-content list-style-type:none}ul.lst-kix_list_2-5{.wiki-html-content list-style-type:none}.lst-kix_list_1-3 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_1-4 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_1-7 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_1-5 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_1-6 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_2-0 > li:before{.wiki-html-content content:"● "}.lst-kix_list_2-1 > li:before{.wiki-html-content content:"○ "}.lst-kix_list_1-8 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_2-2 > li:before{.wiki-html-content content:"■ "}.lst-kix_list_2-3 > li:before{content:"■ "}
                </style>
                
                <h2>1. 서론: 트랜스포머의 유산과 구글의 새로운 해답</h2>
                <p>2017년 구글의 "Attention Is All You Need"는 AI 시대를 열었으나, 2025년에 이르러 트랜스포머 아키텍처는 <strong>2차 복잡도(Quadratic Complexity)</strong>와 <strong>고정된 가중치(Static Weights)</strong>라는 두 가지 한계에 직면했습니다. 구글은 이를 극복하기 위해 단순히 모델 크기를 키우는 것이 아니라, 아키텍처의 근본적인 메커니즘을 재설계하는 데 집중하고 있습니다.</p>
                <p>본 보고서는 구글 리서치(Google Research)와 구글 딥마인드(Google DeepMind)가 발표한 차세대 기술 중, Titans, Mixture-of-Depths, Infini-attention, JEST, Griffin/Hawk 등 검증된 핵심 연구만을 다룹니다.</p>

                <h2>2. Titans: 추론 시점 암기(Test-Time Memorization)와 신경 기억</h2>
                <p>2024년 12월 말 구글 리서치가 공개한 Titans는 트랜스포머의 가장 강력한 대안으로 꼽힙니다. 이 연구는 외부 연구인 'Test-Time Training (TTT)'과 개념적으로 유사하지만, 구글만의 독자적인 '신경 장기 기억(Neural Long-term Memory)' 모듈을 통해 차별화된 성능을 입증했습니다.</p>
                
                <h3>2.1. 핵심 메커니즘: 놀라움(Surprise)과 망각</h3>
                <p>Titans는 모델이 추론(Inference)하는 동안에도 정보를 '기억(학습)'합니다. 이는 기존 트랜스포머가 추론 시점에는 학습된 지식을 고정적으로 사용하는 것과 대조적입니다.</p>
                <ul>
                    <li><strong>놀라움(Surprise) 지표:</strong> Titans는 들어오는 정보가 얼마나 예상 밖인지(Gradient)를 측정합니다. 모델이 예측하지 못한 '놀라운' 정보만이 장기 기억 모듈의 가중치 업데이트에 반영됩니다.</li>
                    <li><strong>적응형 망각(Adaptive Forgetting):</strong> 모든 정보를 기억할 수는 없으므로, <strong>가중치 감쇠(Weight Decay)</strong>와 게이팅 메커니즘을 통해 덜 중요한 정보는 점진적으로 잊혀지게 합니다. 이는 인간의 기억 처리 방식과 유사합니다.</li>
                </ul>

                <h3>2.2. 세 가지 아키텍처 변형 (Variants)</h3>
                <div class="overflow-x-auto my-4">
                    <table class="min-w-full border border-gray-300">
                        <thead class="bg-gray-100">
                            <tr>
                                <th class="border p-2">변형 모델</th>
                                <th class="border p-2">특징</th>
                                <th class="border p-2">구글의 연구 결과</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td class="border p-2 font-semibold">MAC (Memory as a Context)</td>
                                <td class="border p-2">메모리를 추가적인 문맥(Context)으로 취급</td>
                                <td class="border p-2">긴 문맥 처리에 유리하며 RMT(Recurrent Memory Transformer)의 확장 개념</td>
                            </tr>
                            <tr>
                                <td class="border p-2 font-semibold">MAG (Memory as a Gate)</td>
                                <td class="border p-2">게이팅 메커니즘으로 메모리 혼합</td>
                                <td class="border p-2">메모리와 주 연산 경로를 비선형적으로 결합하여 정보 흐름 제어</td>
                            </tr>
                            <tr>
                                <td class="border p-2 font-semibold">MAL (Memory as a Layer)</td>
                                <td class="border p-2">메모리를 하나의 층(Layer)으로 통합</td>
                                <td class="border p-2">가장 우수한 성능. 메모리 모듈 자체가 심층 신경망의 연산 층으로 기능함</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <h3>2.3. 성과 및 의의</h3>
                <p>Titans는 200만 토큰 이상의 문맥 창(Context Window)을 처리하면서도 기존 트랜스포머보다 메모리 효율이 뛰어납니다. 특히 구글은 Titans가 외부의 선형 순환 모델(Linear RNNs)이나 Mamba 아키텍처보다 긴 문맥에서의 '바늘 찾기(Needle-in-a-Haystack)' 성능이 우수함을 입증했습니다.</p>

                <h2>3. Mixture-of-Depths (MoD): 토큰별 동적 연산 배분</h2>
                <p><strong>Mixture-of-Depths (MoD)</strong>는 구글 딥마인드가 2024년 4월 발표한 기술로, 트랜스포머의 비효율적인 연산 자원 배분 문제를 해결했습니다. 모든 토큰에 동일한 계산을 수행하는 대신, 중요한 토큰에만 집중합니다.</p>
                
                <h3>3.1. 라우팅(Routing)과 IsoFLOP 분석</h3>
                <ul>
                    <li><strong>Top-k 라우팅:</strong> 각 레이어의 라우터가 토큰의 중요도를 평가하여, 상위 $k$개의 토큰(예: 전체의 12.5%)만 셀프 어텐션과 MLP 연산을 수행합니다.</li>
                    <li><strong>잔차 연결(Residual bypass):</strong> 선택받지 못한 토큰은 연산을 건너뛰고 잔차 연결을 통해 다음 레이어로 전달됩니다. 이를 통해 정보의 흐름은 유지하되 연산량은 획기적으로 줄입니다.</li>
                </ul>

                <h3>3.2. MoDE: 깊이와 전문가의 결합</h3>
                <p>구글은 MoD를 기존의 MoE(Mixture-of-Experts)와 결합한 MoDE 아키텍처도 제안했습니다. 이는 토큰을 '처리할지 말지(Depth)'와 '어떤 전문가에게 보낼지(Expert)'를 동시에 결정하여 효율성을 극대화합니다. 이 기술은 구글의 Gemini 1.5 및 이후 모델의 효율성 기반이 된 것으로 추정됩니다.</p>

                <h2>4. Infini-attention: 무한한 문맥과 압축 메모리</h2>
                <p>구글 연구진(Tsendsuren Munkhdalai et al.)이 제안한 Infini-attention은 유계 메모리(Bounded Memory) 내에서 무한한 길이의 문맥을 처리하는 기술입니다.</p>
                
                <h3>4.1. 압축 메모리(Compressive Memory) 기술</h3>
                <p>기존 트랜스포머가 과거의 KV(Key-Value) 캐시를 모두 저장하다가 메모리 부족을 겪는 것과 달리, Infini-attention은 오래된 정보를 압축 메모리에 통합합니다.</p>
                <ul>
                    <li><strong>로컬 + 글로벌 하이브리드:</strong> 현재의 문맥은 표준적인 로컬 어텐션(Local Attention)으로 정밀하게 처리하고, 과거의 방대한 문맥은 선형 어텐션(Linear Attention) 기반의 압축 메모리에서 인출(Retrieval)합니다.</li>
                </ul>
                <p><strong>메모리 인출 수식:</strong></p>
                <p>$$A_{mem} = \frac{\sigma(Q) M_{s-1}}{\sigma(Q) z_{s-1}}$$</p>
                <p>이 방식은 10억 파라미터(1B) 규모의 작은 모델로도 100만 토큰 길이의 'Passkey' 테스트를 통과하게 했습니다.</p>

                <h2>5. JEST: 데이터 선별의 가속화</h2>
                <p>모델 아키텍처뿐만 아니라 데이터 학습 방식에서도 구글 딥마인드는 <strong>JEST (Joint Example Selection and Training)</strong>를 통해 혁신을 이루었습니다.</p>
                
                <h3>5.1. 학습 가능성(Learnability) 기반의 배치 선택</h3>
                <p>JEST는 개별 데이터가 아니라 배치(Batch) 단위의 데이터 구성을 최적화합니다. 이를 위해 두 가지 모델을 사용합니다.</p>
                <ol>
                    <li><strong>참조 모델(Reference Model):</strong> 양질의 데이터로 사전 학습된 모델.</li>
                    <li><strong>학습 모델(Learner Model):</strong> 현재 학습 중인 모델.</li>
                </ol>
                <p>JEST는 "참조 모델은 잘 알지만(고품질), 학습 모델은 아직 모르는(높은 손실값)" 데이터 배치를 우선적으로 선별합니다.</p>

                <h3>5.2. 효율성: 13배 빠른 학습</h3>
                <p>이 방식은 무작위 배치 학습 대비 <strong>13배 더 적은 반복(Iteration)</strong>과 <strong>10배 더 적은 연산(Compute)</strong>만으로 동등한 성능에 도달했습니다. 이는 '데이터의 질'을 알고리즘적으로 평가하여 AI 학습 속도를 비약적으로 높인 구글 딥마인드의 핵심 성과입니다.</p>

                <h2>6. Griffin & Hawk: RNN의 현대적 재해석</h2>
                <p>구글 딥마인드는 트랜스포머의 대안으로 Griffin과 Hawk를 발표하며 순환신경망(RNN)을 부활시켰습니다.</p>
                
                <h3>6.1. RG-LRU (Real-Gated Linear Recurrent Unit)</h3>
                <p>이들 모델의 핵심은 RG-LRU라는 새로운 순환 유닛입니다. 이는 하드웨어 효율적인 요소별(Element-wise) 연산을 사용하여, RNN의 단점인 느린 학습 속도를 극복하고 트랜스포머 수준의 학습 효율을 달성했습니다.</p>
                <ul>
                    <li><strong>Hawk:</strong> 순수 RNN 기반 모델.</li>
                    <li><strong>Griffin:</strong> RG-LRU와 로컬 어텐션을 결합한 하이브리드 모델.</li>
                </ul>
                <p>Griffin은 Llama-2와 같은 트랜스포머 모델과 대등한 성능을 보이면서도, 추론 시에는 훨씬 적은 메모리와 빠른 속도를 제공합니다.</p>

                <h2>7. Gemini 2.0과 구글의 통합 전략</h2>
                <p>구글의 이러한 개별 연구들은 최신 플래그십 모델인 Gemini 2.0 시리즈에 통합되고 있습니다.</p>
                <ul>
                    <li><strong>Test-Time Learning의 적용:</strong> Gemini 2.0 Flash와 같은 모델은 에이전트 작업 수행 시, 환경 피드백을 통해 추론 과정에서 전략을 수정하는 능력을 보여줍니다. 이는 Titans 연구에서 제안된 '추론 시점 학습/암기' 개념이 상용 모델에 적용되고 있음을 시사합니다.</li>
                    <li><strong>무한 문맥과 멀티모달:</strong> 200만 토큰 이상의 긴 문맥 처리 능력은 Infini-attention과 MoD의 효율적인 연산 관리 기술 없이는 구현이 불가능에 가깝습니다.</li>
                </ul>

                <h2>8. 결론</h2>
                <p>구글은 '트랜스포머 이후(Post-Transformer)'를 대비하여, 기억(Titans), 효율(MoD, JEST), 확장성(Infini-attention), 대안 아키텍처(Griffin) 전반에 걸쳐 독자적인 기술 포트폴리오를 완성했습니다. 특히 Titans의 등장은 AI가 정적인 함수에서 벗어나, 실시간으로 경험을 기억하고 적응하는 '동적인 지능'으로 진화하고 있음을 보여주는 구글의 가장 중요한 'Next Attention' 모멘트입니다.</p>
            </div>
            
            <div class="mt-12 pt-4 border-t border-[#a2a9b1] text-xs text-[#444] font-medium italic">
                This page was last edited on Dec 30, 2025.
            </div>
        </article>
    </div>
</body>
</html>